import streamlit as st
import hashlib
import tempfile
import os
import numpy as np
from typing import List, Tuple
from sentence_transformers import SentenceTransformer
from pypdf import PdfReader  # For PDF text extraction (fallback)
from openai import OpenAI  # For GPT API calls
import fitz  # PyMuPDF

# Initialize OpenAI client
client = OpenAI(api_key=st.secrets["OPENAI_API_KEY"])

# Streamlit UI
st.set_page_config(page_title="Assistant - SÃ©curitÃ© RoutiÃ¨re", layout="centered")
st.title("ðŸ“„ Posez des questions sur la sÃ©curitÃ© RoutiÃ¨re")

# Default system message
default_system_msg = """**Expert SÃ©curitÃ© RoutiÃ¨re Strict**  

*Mission* : RÃ©pondre EXCLUSIVEMENT aux questions sur la sÃ©curitÃ© routiÃ¨re avec une expertise maximale.  

**RÃ¨gles Absolues** :  
1. **Filtre ThÃ©matique** :  
   - Si la question ne concerne PAS la sÃ©curitÃ© routiÃ¨re et code de la route â†’ RÃ©ponse systÃ©matique :  
     *"DÃ©solÃ©, je suis strictement limitÃ© aux questions de sÃ©curitÃ© routiÃ¨re. Je ne peux pas rÃ©pondre Ã  cette demande."*  
   - Aucune exception (mÃªme pour les questions connexes comme mÃ©canique ou GPS)  

2. **PrioritÃ© Documentaire** :  
   - "Le document indique que..." pour toute information sourcÃ©e  
   - Croiser les informations du document avec le Code de la Route officiel  
   - Marquer clairement : [SOURCE DOCUMENT] / [CONNAISSANCE EXPERTE]  

3. **PrÃ©cision Technique** :  
   - Chiffres exacts (ex : distances de freinage, taux d'alcoolÃ©mie)  
   - Citations prÃ©cises des articles du Code de la Route quand applicable  
   - Mises Ã  jour lÃ©gislatives rÃ©centes  

4. **Format de RÃ©ponse** :  
   â–¶ **RÃ©ponse Directe** : 1-2 phrases maximum  
   â–¶ **Explications** : Bullet points techniques si nÃ©cessaire  
   â–¶ **Sources** :  
       - Document fourni (page X si disponible)  
       - Article R.XXX du Code de la Route  
   â–¶ **Avertissements** : "VÃ©rifiez toujours les textes officiels"  

**Interdictions** :  
- Pas de hors-sujet  
- Pas de conseils personnalisÃ©s (ex : "Dans votre cas...")  
- Pas d'interprÃ©tation libre des lois"""

# File to load
pdf_path = "nosia.pdf"

# Session state setup
if 'system_msg' not in st.session_state:
    st.session_state.system_msg = default_system_msg

# PDF processing
@st.cache_resource(show_spinner="Chargement du modÃ¨le d'embedding...")
def load_embedding_model() -> SentenceTransformer:
    return SentenceTransformer("paraphrase-multilingual-mpnet-base-v2")

@st.cache_data(show_spinner="Analyse du PDF en cours...")
def process_pdf(file_path: str) -> Tuple[List[str], np.ndarray]:
    with open(file_path, "rb") as f:
        file_bytes = f.read()
    text = extract_text_from_pdf(file_bytes)
    chunks = chunk_text(text)
    embedder = load_embedding_model()
    embeddings = embedder.encode(chunks, convert_to_numpy=True)
    return chunks, embeddings

def extract_text_from_pdf(file_bytes: bytes) -> str:
    try:
        doc = fitz.open(stream=file_bytes, filetype="pdf")
        text = "\n".join([page.get_text() for page in doc])
        if text.strip():
            return text
    except:
        pass
    with tempfile.NamedTemporaryFile(delete=False, suffix=".pdf") as tmp_file:
        tmp_file.write(file_bytes)
        tmp_path = tmp_file.name
    try:
        reader = PdfReader(tmp_path)
        text = " ".join(page.extract_text() or "" for page in reader.pages)
        return text if text.strip() else ""
    finally:
        os.unlink(tmp_path)

def chunk_text(text: str, chunk_size=512, overlap=64) -> List[str]:
    words = text.split()
    chunks = []
    for i in range(0, len(words), chunk_size - overlap):
        chunk = ' '.join(words[i:i+chunk_size])
        chunks.append(chunk)
    return chunks

def search_chunks(query: str, chunks: List[str], embeddings: np.ndarray, k=5) -> List[Tuple[str, float]]:
    embedder = load_embedding_model()
    query_embedding = embedder.encode([query], convert_to_numpy=True)
    similarities = np.dot(embeddings, query_embedding.T).flatten()
    top_indices = np.argsort(similarities)[-k:][::-1]
    return [(chunks[i], float(similarities[i])) for i in top_indices if i < len(chunks)]

def generate_response(query: str, relevant_chunks: List[Tuple[str, float]], system_msg: str, context: str):
    prompt = f"""QUESTION DE L'UTILISATEUR : {query}

{context}

ANALYSE ET RÃ‰PONSE :
1. Cette question concerne-t-elle le document ou la rÃ©glementation routiÃ¨re ?
2. Quelles informations pertinentes existent dans le document ?
3. Quelles connaissances complÃ©mentaires peuvent Ãªtre ajoutÃ©es avec prudence ?
4. Formulez la rÃ©ponse la plus prÃ©cise et sÃ©curitaire."""

    response = client.chat.completions.create(
        model="gpt-4-turbo",
        messages=[
            {"role": "system", "content": system_msg},
            {"role": "user", "content": prompt}
        ],
        temperature=0.3,
        max_tokens=500
    )
    return response.choices[0].message.content

# Load and cache chunks/embeddings
if os.path.exists(pdf_path) and 'chunks' not in st.session_state:
    chunks, embeddings = process_pdf(pdf_path)
    st.session_state.chunks = chunks
    st.session_state.embeddings = embeddings

# Form input
with st.form(key='query_form'):
    # Full-width input for system message
    system_msg_input = st.text_area(
        "Modifiez le message du systÃ¨me (rÃ¨gles pour l'Assistant)",
        value=st.session_state.system_msg,
        height=200,
        key="system_msg_input"
    )

    if st.form_submit_button("Afficher le message par dÃ©faut"):
        st.info("Message systÃ¨me par dÃ©faut :")
        st.code(default_system_msg, language="markdown")

    # Question input and submit
    query = st.text_input("Posez votre question sur le document:")
    submitted = st.form_submit_button("Soumettre la question")

# Handle query submission
if submitted and query:
    with st.spinner("Recherche des rÃ©ponses..."):
        relevant_chunks = search_chunks(query, st.session_state.chunks, st.session_state.embeddings)

        if not relevant_chunks:
            st.warning("Aucune information pertinente trouvÃ©e dans le document.")
        else:
            # Context is now dynamically generated from chunks
            context = "\n\n---\n\n".join([
                f"EXTRAIT DOCUMENT (Pertinence: {score:.2f}):\n{text}"
                for text, score in relevant_chunks
            ])

            # Update session system message
            st.session_state.system_msg = system_msg_input

            answer = generate_response(query, relevant_chunks, st.session_state.system_msg, context)
            st.success(answer)

            with st.expander("Voir les passages pertinents"):
                for chunk, score in relevant_chunks:
                    st.write(f"Pertinence: {score:.2f}")
                    st.text(chunk)
                    st.divider()
